{"file_contents":{"README.md":{"content":"# GPU-Accelerated Face Recognition System\n\nA high-performance Python face recognition system using ArcFace embeddings and FAISS for efficient similarity matching. The system supports both GPU acceleration and CPU fallback, making it suitable for various deployment scenarios.\n\n## Features\n\n- **ArcFace Embeddings**: Uses InsightFace's buffalo_l model for 512-dimensional face embeddings\n- **GPU Acceleration**: Automatic GPU detection with CPU fallback\n- **FAISS Integration**: Efficient similarity search for large-scale face matching\n- **Batch Processing**: Optimized for processing up to 1000+ images\n- **Modular Architecture**: Clean, testable code structure\n- **Progress Tracking**: Real-time progress indicators and detailed logging\n\n## System Requirements\n\n- Python 3.7+\n- OpenCV (cv2)\n- NumPy\n- InsightFace\n- FAISS (CPU or GPU version)\n- ONNXRuntime\n\n## Installation\n\nThe required dependencies are already installed in this environment:\n- `insightface` - Face detection and embedding extraction\n- `faiss-cpu` - Similarity search (CPU version)\n- `opencv-python` - Image processing\n- `numpy` - Numerical operations\n- `onnxruntime` - Model inference\n\n## Quick Start\n\n### 1. Check System Information\n\n```bash\npython main.py --system-info\n```\n\nThis displays GPU availability, FAISS support, and InsightFace status.\n\n### 2. Basic Usage\n\n```bash\npython main.py --folder /path/to/images --selfie /path/to/selfie.jpg\n```\n\n### 3. Advanced Options\n\n```bash\npython main.py --folder ./photos --selfie ./query.jpg --max-matches 20 --threshold 0.7 --max-images 500\n```\n\n## Command Line Arguments\n\n- `--folder`: Directory containing images to process\n- `--selfie`: Path to the query image for matching\n- `--max-images`: Maximum number of images to process (default: unlimited)\n- `--max-matches`: Maximum number of matches to return (default: 10)\n- `--threshold`: Similarity threshold for matches (default: 0.6)\n- `--system-info`: Display system information and exit\n\n## How It Works\n\n1. **Image Loading**: Processes all supported image formats (.jpg, .jpeg, .png, .bmp, .tiff, .webp)\n2. **Face Detection**: Uses InsightFace to detect faces in each image\n3. **Embedding Extraction**: Generates 512-dimensional ArcFace embeddings\n4. **Database Storage**: Stores embeddings with metadata in memory\n5. **Index Building**: Creates FAISS index for efficient similarity search\n6. **Query Processing**: Extracts embedding from selfie image\n7. **Similarity Matching**: Finds closest matches using cosine similarity\n8. **Results Display**: Shows matching images with similarity scores\n\n## Architecture\n\nThe system follows a modular design:\n\n- **config.py**: Configuration management\n- **utils.py**: Utility functions and system checks\n- **database.py**: In-memory face database\n- **face_processor.py**: Face detection and embedding extraction\n- **similarity_matcher.py**: FAISS-based similarity matching\n- **main.py**: Main orchestration and CLI interface\n\n## Performance\n\n- **CPU Processing**: ~2-5 images/second (depending on image size)\n- **GPU Processing**: ~10-20 images/second (with CUDA support)\n- **Memory Usage**: ~1-2 GB for 1000 face embeddings\n- **Search Speed**: Sub-second for queries against 10,000+ faces\n\n## Configuration\n\nKey settings in `config.py`:\n\n```python\nARCFACE_MODEL_NAME = 'buffalo_l'           # ArcFace model\nEMBEDDING_DIMENSION = 512                   # Embedding size\nSIMILARITY_THRESHOLD = 0.6                  # Match threshold\nMAX_IMAGES_TO_PROCESS = 1000               # Processing limit\nBATCH_SIZE = 32                            # Batch processing size\n```\n\n## GPU Support\n\nThe system automatically detects and uses GPU acceleration when available:\n\n- **CUDA**: For InsightFace model inference\n- **FAISS-GPU**: For similarity search acceleration\n\nIf GPU is not available, it gracefully falls back to CPU processing.\n\n## Error Handling\n\nThe system includes comprehensive error handling:\n\n- Invalid image formats are skipped with warnings\n- Face detection failures are logged but don't stop processing\n- GPU initialization failures trigger CPU fallback\n- Memory constraints are monitored and reported\n\n## Limitations\n\n- Requires at least one face per image for processing\n- Multiple faces per image: uses the largest detected face\n- Works best with frontal face views\n- Minimum face size: 50x50 pixels for reliable detection\n\n## Examples\n\n### Process a folder of event photos\n```bash\npython main.py --folder ./event_photos --selfie ./my_selfie.jpg\n```\n\n### Find matches with custom threshold\n```bash\npython main.py --folder ./images --selfie ./query.jpg --threshold 0.8\n```\n\n### Limit processing for testing\n```bash\npython main.py --folder ./large_dataset --selfie ./test.jpg --max-images 100\n```\n\n## Troubleshooting\n\n1. **\"No faces detected\"**: Ensure images contain clear, frontal faces\n2. **Slow processing**: Check if GPU acceleration is working with `--system-info`\n3. **Memory issues**: Reduce `MAX_IMAGES_TO_PROCESS` or `BATCH_SIZE`\n4. **Import errors**: Verify all dependencies are installed correctly\n\n## License\n\nThis project demonstrates face recognition capabilities using open-source libraries. Ensure compliance with your local data protection and privacy regulations.","size_bytes":5200},"app.py":{"content":"\"\"\"\nFlask web application for FaceAlert Admin Dashboard.\n\"\"\"\nimport os\nimport cv2\nimport numpy as np\nfrom datetime import datetime\nfrom flask import Flask, render_template, request, jsonify, session, redirect, url_for, send_file\nfrom werkzeug.utils import secure_filename\nfrom functools import wraps\nfrom face_processor import FaceProcessor\nfrom embeddings_store import EmbeddingsStore\nfrom google_drive_helper import upload_file_to_drive, download_file_from_drive, delete_file_from_drive, list_files_in_folder, download_file_to_memory\n\napp = Flask(__name__)\napp.secret_key = os.environ.get('FLASK_SECRET_KEY', 'dev-secret-key-change-me')\napp.config['MAX_CONTENT_LENGTH'] = 100 * 1024 * 1024\napp.config['UPLOAD_FOLDER'] = 'uploads'\n\nos.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)\nos.makedirs(os.path.join(app.config['UPLOAD_FOLDER'], 'photos'), exist_ok=True)\nos.makedirs(os.path.join(app.config['UPLOAD_FOLDER'], 'videos'), exist_ok=True)\nos.makedirs(os.path.join(app.config['UPLOAD_FOLDER'], 'temp'), exist_ok=True)\n\n# Initialize face recognition system\nface_processor = None\nembeddings_store = EmbeddingsStore()\n\ndef get_face_processor():\n    \"\"\"Lazy load face processor to avoid startup delays.\"\"\"\n    global face_processor\n    if face_processor is None:\n        face_processor = FaceProcessor()\n    return face_processor\n\nALLOWED_IMAGE_EXTENSIONS = {'png', 'jpg', 'jpeg', 'gif'}\nALLOWED_VIDEO_EXTENSIONS = {'mp4', 'mov', 'avi', 'webm'}\n\nADMIN_EMAIL = os.environ.get('ADMIN_EMAIL', 'admin@facealert.com')\nADMIN_PASSWORD = os.environ.get('ADMIN_PASSWORD', 'admin123')\n\nGOOGLE_DRIVE_FOLDER_ID = '1eoM3z7eWgGmLsFM9rhuL17zsB8Cd6yUq'\n\nreports = [\n    {\n        'id': 1,\n        'photo_url': 'https://images.unsplash.com/photo-1507003211169-0a1dd7228f2d?w=400&h=400&fit=crop',\n        'video_url': None,\n        'upload_date': '2025-09-28 14:30',\n        'status': 'Pending'\n    },\n    {\n        'id': 2,\n        'photo_url': 'https://images.unsplash.com/photo-1494790108377-be9c29b29330?w=400&h=400&fit=crop',\n        'video_url': None,\n        'upload_date': '2025-09-29 09:15',\n        'status': 'Pending'\n    },\n    {\n        'id': 3,\n        'photo_url': 'https://images.unsplash.com/photo-1500648767791-00dcc994a43e?w=400&h=400&fit=crop',\n        'video_url': None,\n        'upload_date': '2025-09-29 16:45',\n        'status': 'Reviewed'\n    }\n]\n\ndef login_required(f):\n    @wraps(f)\n    def decorated_function(*args, **kwargs):\n        if 'logged_in' not in session:\n            return redirect(url_for('login'))\n        return f(*args, **kwargs)\n    return decorated_function\n\ndef allowed_file(filename, file_type='image'):\n    if '.' not in filename:\n        return False\n    ext = filename.rsplit('.', 1)[1].lower()\n    if file_type == 'image':\n        return ext in ALLOWED_IMAGE_EXTENSIONS\n    elif file_type == 'video':\n        return ext in ALLOWED_VIDEO_EXTENSIONS\n    return False\n\n@app.route('/')\ndef index():\n    return redirect(url_for('report_page'))\n\n@app.route('/report')\ndef report_page():\n    return render_template('report.html')\n\n@app.route('/report/submit', methods=['POST'])\ndef submit_report():\n    temp_path = None\n    downloaded_path = None\n    drive_file_id = None\n    \n    try:\n        if 'photo' not in request.files:\n            return jsonify({'error': 'No photo provided'}), 400\n        \n        photo = request.files['photo']\n        \n        if photo.filename == '':\n            return jsonify({'error': 'No photo selected'}), 400\n        \n        if not allowed_file(photo.filename, 'image'):\n            return jsonify({'error': 'Invalid photo format'}), 400\n        \n        print(\"Step 1: Saving uploaded photo...\")\n        photo_filename = secure_filename(photo.filename)\n        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n        photo_filename = f\"{timestamp}_{photo_filename}\"\n        temp_path = os.path.join(app.config['UPLOAD_FOLDER'], 'temp', photo_filename)\n        photo.save(temp_path)\n        \n        # Step 2: Upload to Google Drive\n        print(\"Step 2: Uploading to Google Drive...\")\n        drive_file_id = upload_file_to_drive(temp_path, photo_filename)\n        \n        # Step 3: Download back from Google Drive\n        print(\"Step 3: Downloading from Google Drive...\")\n        downloaded_path = os.path.join(app.config['UPLOAD_FOLDER'], 'temp', f\"downloaded_{photo_filename}\")\n        download_success = download_file_from_drive(drive_file_id, downloaded_path)\n        \n        if not download_success:\n            raise Exception(\"Failed to download from Google Drive\")\n        \n        # Step 4: Extract face embedding using InsightFace\n        print(\"Step 4: Extracting face embedding...\")\n        processor = get_face_processor()\n        \n        # Load image\n        image = cv2.imread(downloaded_path)\n        if image is None:\n            raise Exception(\"Failed to load downloaded image\")\n        \n        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        # Extract embedding\n        embedding = processor.extract_embedding(image_rgb)\n        \n        if embedding is None:\n            return jsonify({\n                'found': False,\n                'score': 0.0,\n                'message': 'No face detected in the photo'\n            })\n        \n        # Step 5: Compare with stored embeddings\n        print(\"Step 5: Comparing with stored embeddings...\")\n        found, score, person_data = embeddings_store.find_match(embedding, threshold=0.6)\n        \n        # Step 6: Compare with images from Google Drive folder\n        print(\"Step 6: Comparing with Google Drive images...\")\n        matched_drive_images = []\n        MAX_DRIVE_FILES_TO_PROCESS = 50\n        \n        try:\n            drive_files = list_files_in_folder(GOOGLE_DRIVE_FOLDER_ID)\n            print(f\"Found {len(drive_files)} images in Google Drive folder (processing max {MAX_DRIVE_FILES_TO_PROCESS})\")\n            \n            for idx, drive_file in enumerate(drive_files[:MAX_DRIVE_FILES_TO_PROCESS]):\n                try:\n                    file_bytes = download_file_to_memory(drive_file['id'])\n                    if file_bytes:\n                        nparr = np.frombuffer(file_bytes, np.uint8)\n                        drive_image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n                        \n                        if drive_image is not None:\n                            drive_image_rgb = cv2.cvtColor(drive_image, cv2.COLOR_BGR2RGB)\n                            drive_embedding = processor.extract_embedding(drive_image_rgb)\n                            \n                            if drive_embedding is not None:\n                                similarity = np.dot(embedding, drive_embedding) / (\n                                    np.linalg.norm(embedding) * np.linalg.norm(drive_embedding)\n                                )\n                                \n                                if similarity > 0.6:\n                                    matched_drive_images.append({\n                                        'file_id': drive_file['id'],\n                                        'name': drive_file['name'],\n                                        'similarity': float(similarity)\n                                    })\n                                    print(f\"Match found: {drive_file['name']} with similarity {similarity:.2f}\")\n                except Exception as e:\n                    print(f\"Error processing drive file {drive_file.get('name', 'unknown')}: {str(e)}\")\n                    continue\n            \n            matched_drive_images.sort(key=lambda x: x['similarity'], reverse=True)\n            \n        except Exception as e:\n            print(f\"Error comparing with Drive images: {str(e)}\")\n        \n        # Save photo for admin review\n        final_photo_path = os.path.join(app.config['UPLOAD_FOLDER'], 'photos', photo_filename)\n        os.rename(temp_path, final_photo_path)\n        temp_path = None\n        \n        # Add to reports\n        new_report = {\n            'id': max([r['id'] for r in reports]) + 1 if reports else 1,\n            'photo_url': f'/uploads/photos/{photo_filename}',\n            'video_url': None,\n            'upload_date': datetime.now().strftime('%Y-%m-%d %H:%M'),\n            'status': 'Pending',\n            'drive_file_id': drive_file_id,\n            'match_found': found or len(matched_drive_images) > 0,\n            'match_score': score,\n            'matched_person': person_data['name'] if person_data else None,\n            'drive_matches': matched_drive_images\n        }\n        \n        reports.insert(0, new_report)\n        \n        # Step 7: Return JSON response\n        drive_top_score = matched_drive_images[0]['similarity'] if matched_drive_images else 0.0\n        display_score = max(score, drive_top_score)\n        \n        response_data = {\n            'found': found or len(matched_drive_images) > 0,\n            'score': round(display_score, 2),\n            'drive_matches': matched_drive_images[:5]\n        }\n        \n        if found and person_data:\n            response_data['person'] = {\n                'name': person_data['name'],\n                'id': person_data['id']\n            }\n        \n        print(f\"Result: Found={found}, Score={score}, Drive Top Score={drive_top_score}, Drive Matches={len(matched_drive_images)}\")\n        return jsonify(response_data)\n        \n    except Exception as e:\n        print(f\"Error in submit_report: {str(e)}\")\n        \n        # Cleanup on error\n        if drive_file_id:\n            try:\n                delete_file_from_drive(drive_file_id)\n            except:\n                pass\n        \n        return jsonify({'error': str(e)}), 500\n        \n    finally:\n        # Cleanup temporary files\n        if temp_path and os.path.exists(temp_path):\n            os.remove(temp_path)\n        if downloaded_path and os.path.exists(downloaded_path):\n            os.remove(downloaded_path)\n\n@app.route('/admin/login', methods=['GET'])\ndef login():\n    return redirect(url_for('dashboard'))\n\n@app.route('/admin/login', methods=['POST'])\ndef login_post():\n    data = request.json\n    email = data.get('email', '')\n    password = data.get('password', '')\n    \n    if email == ADMIN_EMAIL and password == ADMIN_PASSWORD:\n        session['logged_in'] = True\n        session['email'] = email\n        return jsonify({'success': True})\n    \n    return jsonify({'error': 'Invalid email or password'}), 401\n\n@app.route('/admin/logout', methods=['POST'])\ndef logout():\n    session.clear()\n    return jsonify({'success': True})\n\n@app.route('/admin/dashboard')\ndef dashboard():\n    return render_template('dashboard.html', reports=reports)\n\n@app.route('/admin/update-report', methods=['POST'])\ndef update_report():\n    data = request.json\n    report_id = data.get('id')\n    new_status = data.get('status')\n    \n    for report in reports:\n        if report['id'] == report_id:\n            report['status'] = new_status\n            return jsonify({'success': True})\n    \n    return jsonify({'error': 'Report not found'}), 404\n\n@app.route('/admin/upload-report', methods=['POST'])\ndef upload_report():\n    try:\n        if 'photo' not in request.files:\n            return jsonify({'error': 'No photo provided'}), 400\n        \n        photo = request.files['photo']\n        video = request.files.get('video')\n        \n        if photo.filename == '':\n            return jsonify({'error': 'No photo selected'}), 400\n        \n        if not allowed_file(photo.filename, 'image'):\n            return jsonify({'error': 'Invalid photo format'}), 400\n        \n        photo_filename = secure_filename(photo.filename)\n        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n        photo_filename = f\"{timestamp}_{photo_filename}\"\n        photo_path = os.path.join(app.config['UPLOAD_FOLDER'], 'photos', photo_filename)\n        photo.save(photo_path)\n        \n        video_url = None\n        if video and video.filename != '':\n            if allowed_file(video.filename, 'video'):\n                video_filename = secure_filename(video.filename)\n                video_filename = f\"{timestamp}_{video_filename}\"\n                video_path = os.path.join(app.config['UPLOAD_FOLDER'], 'videos', video_filename)\n                video.save(video_path)\n                video_url = f'/uploads/videos/{video_filename}'\n        \n        new_report = {\n            'id': max([r['id'] for r in reports]) + 1 if reports else 1,\n            'photo_url': f'/uploads/photos/{photo_filename}',\n            'video_url': video_url,\n            'upload_date': datetime.now().strftime('%Y-%m-%d %H:%M'),\n            'status': 'Pending'\n        }\n        \n        reports.insert(0, new_report)\n        \n        return jsonify({'success': True, 'report': new_report})\n        \n    except Exception as e:\n        return jsonify({'error': str(e)}), 500\n\n@app.route('/admin/add-person', methods=['POST'])\ndef add_person():\n    \"\"\"Add a known person to the embeddings database.\"\"\"\n    temp_path = None\n    \n    try:\n        if 'photo' not in request.files:\n            return jsonify({'error': 'No photo provided'}), 400\n        \n        name = request.form.get('name', '').strip()\n        if not name:\n            return jsonify({'error': 'Person name is required'}), 400\n        \n        photo = request.files['photo']\n        \n        if photo.filename == '':\n            return jsonify({'error': 'No photo selected'}), 400\n        \n        if not allowed_file(photo.filename, 'image'):\n            return jsonify({'error': 'Invalid photo format'}), 400\n        \n        photo_filename = secure_filename(photo.filename)\n        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n        photo_filename = f\"{timestamp}_{photo_filename}\"\n        temp_path = os.path.join(app.config['UPLOAD_FOLDER'], 'temp', photo_filename)\n        photo.save(temp_path)\n        \n        processor = get_face_processor()\n        \n        image = cv2.imread(temp_path)\n        if image is None:\n            return jsonify({'error': 'Failed to load image'}), 400\n        \n        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        embedding = processor.extract_embedding(image_rgb)\n        \n        if embedding is None:\n            return jsonify({'error': 'No face detected in the photo'}), 400\n        \n        final_photo_path = os.path.join(app.config['UPLOAD_FOLDER'], 'photos', photo_filename)\n        os.rename(temp_path, final_photo_path)\n        temp_path = None\n        \n        person_id = embeddings_store.add_person(\n            name=name,\n            embedding=embedding,\n            photo_url=f'/uploads/photos/{photo_filename}'\n        )\n        \n        return jsonify({\n            'success': True,\n            'person_id': person_id,\n            'name': name,\n            'message': f'Successfully added {name} to database'\n        })\n        \n    except Exception as e:\n        print(f\"Error adding person: {str(e)}\")\n        return jsonify({'error': str(e)}), 500\n    \n    finally:\n        if temp_path and os.path.exists(temp_path):\n            os.remove(temp_path)\n\n@app.route('/admin/get-persons', methods=['GET'])\ndef get_persons():\n    \"\"\"Get all persons in the database.\"\"\"\n    persons = embeddings_store.get_all_persons()\n    return jsonify({'persons': persons, 'count': len(persons)})\n\n@app.route('/uploads/<folder>/<filename>')\ndef serve_upload(folder, filename):\n    filepath = os.path.join(app.config['UPLOAD_FOLDER'], folder, filename)\n    if os.path.exists(filepath):\n        return send_file(filepath)\n    return \"File not found\", 404\n\n@app.route('/drive/image/<file_id>')\ndef serve_drive_image(file_id):\n    \"\"\"Serve an image from Google Drive.\"\"\"\n    try:\n        file_bytes = download_file_to_memory(file_id)\n        if file_bytes:\n            from io import BytesIO\n            return send_file(BytesIO(file_bytes), mimetype='image/jpeg')\n        return \"Image not found\", 404\n    except Exception as e:\n        print(f\"Error serving Drive image: {str(e)}\")\n        return \"Error loading image\", 500\n\n@app.errorhandler(413)\ndef request_entity_too_large(error):\n    return jsonify({'error': 'File too large. Maximum upload size is 100MB.'}), 413\n\nif __name__ == '__main__':\n    debug_mode = os.environ.get('FLASK_DEBUG', 'False').lower() == 'true'\n    app.run(host='0.0.0.0', port=5000, debug=debug_mode)\n","size_bytes":16313},"config.py":{"content":"\"\"\"\nConfiguration settings for the face recognition system.\n\"\"\"\n\nimport os\n\n# Model configuration\nARCFACE_MODEL_NAME = 'buffalo_l'\nEMBEDDING_DIMENSION = 512  # buffalo_l produces 512-dimensional embeddings\nSIMILARITY_THRESHOLD = 0.4  # Minimum similarity score for matches\n\n# GPU configuration\nFORCE_CPU = os.getenv(\"FORCE_CPU\", \"false\").lower() == \"true\"\nGPU_DEVICE_ID = int(os.getenv(\"GPU_DEVICE_ID\", \"0\"))\n\n# Processing configuration\nBATCH_SIZE = 32  # Images to process in a single batch\nMAX_IMAGE_SIZE = 1024  # Maximum image dimension for processing\nMIN_FACE_SIZE = 50  # Minimum face size for detection\n\n# File handling\nSUPPORTED_IMAGE_FORMATS = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp']\nMAX_IMAGES_TO_PROCESS = 1000\n\n# Logging configuration\nLOG_LEVEL = os.getenv(\"LOG_LEVEL\", \"INFO\")\nSHOW_PROGRESS = True\n","size_bytes":823},"database.py":{"content":"\"\"\"\nIn-memory database for storing face embeddings and metadata.\n\"\"\"\n\nimport numpy as np\nfrom typing import List, Dict, Optional, Tuple\nimport logging\nfrom dataclasses import dataclass\nfrom datetime import datetime\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass FaceRecord:\n    \"\"\"Data class for storing face information.\"\"\"\n    id: int\n    image_path: str\n    embedding: np.ndarray\n    face_bbox: Tuple[int, int, int, int]  # x, y, width, height\n    confidence: float\n    timestamp: datetime\n    \n    def to_dict(self) -> Dict:\n        \"\"\"Convert to dictionary representation.\"\"\"\n        return {\n            'id': self.id,\n            'image_path': self.image_path,\n            'embedding_shape': self.embedding.shape,\n            'face_bbox': self.face_bbox,\n            'confidence': self.confidence,\n            'timestamp': self.timestamp.isoformat()\n        }\n\nclass FaceDatabase:\n    \"\"\"In-memory database for face embeddings and metadata.\"\"\"\n    \n    def __init__(self):\n        \"\"\"Initialize the database.\"\"\"\n        self.records: List[FaceRecord] = []\n        self.next_id = 1\n        self._embedding_matrix: Optional[np.ndarray] = None\n        self._needs_rebuild = True\n        \n        logger.info(\"Initialized face database\")\n    \n    def add_face(self, image_path: str, embedding: np.ndarray, \n                 face_bbox: Tuple[int, int, int, int], confidence: float) -> int:\n        \"\"\"\n        Add a face record to the database.\n        \n        Args:\n            image_path: Path to the source image\n            embedding: Face embedding vector\n            face_bbox: Bounding box coordinates (x, y, width, height)\n            confidence: Detection confidence score\n            \n        Returns:\n            Record ID\n        \"\"\"\n        record = FaceRecord(\n            id=self.next_id,\n            image_path=image_path,\n            embedding=embedding.copy(),\n            face_bbox=face_bbox,\n            confidence=confidence,\n            timestamp=datetime.now()\n        )\n        \n        self.records.append(record)\n        self.next_id += 1\n        self._needs_rebuild = True\n        \n        logger.debug(f\"Added face record {record.id} for {image_path}\")\n        return record.id\n    \n    def get_record(self, record_id: int) -> Optional[FaceRecord]:\n        \"\"\"\n        Get a record by ID.\n        \n        Args:\n            record_id: Record ID to retrieve\n            \n        Returns:\n            FaceRecord or None if not found\n        \"\"\"\n        for record in self.records:\n            if record.id == record_id:\n                return record\n        return None\n    \n    def get_all_embeddings(self) -> np.ndarray:\n        \"\"\"\n        Get all embeddings as a matrix.\n        \n        Returns:\n            Matrix of embeddings (n_records x embedding_dim)\n        \"\"\"\n        if not self.records:\n            return np.array([]).reshape(0, 512)  # Empty array with correct shape\n        \n        if self._needs_rebuild or self._embedding_matrix is None:\n            embeddings = [record.embedding for record in self.records]\n            self._embedding_matrix = np.vstack(embeddings)\n            self._needs_rebuild = False\n            logger.debug(f\"Rebuilt embedding matrix: {self._embedding_matrix.shape}\")\n        \n        return self._embedding_matrix\n    \n    def get_image_paths(self) -> List[str]:\n        \"\"\"\n        Get all image paths in order.\n        \n        Returns:\n            List of image paths\n        \"\"\"\n        return [record.image_path for record in self.records]\n    \n    def clear(self):\n        \"\"\"Clear all records from the database.\"\"\"\n        self.records.clear()\n        self.next_id = 1\n        self._embedding_matrix = None\n        self._needs_rebuild = True\n        logger.info(\"Cleared face database\")\n    \n    def size(self) -> int:\n        \"\"\"Get the number of records in the database.\"\"\"\n        return len(self.records)\n    \n    def get_statistics(self) -> Dict:\n        \"\"\"\n        Get database statistics.\n        \n        Returns:\n            Dictionary with statistics\n        \"\"\"\n        if not self.records:\n            return {\n                'total_records': 0,\n                'unique_images': 0,\n                'avg_confidence': 0.0,\n                'embedding_dimension': 0\n            }\n        \n        unique_images = len(set(record.image_path for record in self.records))\n        avg_confidence = np.mean([record.confidence for record in self.records])\n        embedding_dim = self.records[0].embedding.shape[0] if self.records else 0\n        \n        return {\n            'total_records': len(self.records),\n            'unique_images': unique_images,\n            'avg_confidence': float(avg_confidence),\n            'embedding_dimension': embedding_dim\n        }\n    \n    def print_summary(self):\n        \"\"\"Print a summary of the database contents.\"\"\"\n        stats = self.get_statistics()\n        \n        print(\"\\n\" + \"=\"*50)\n        print(\"FACE DATABASE SUMMARY\")\n        print(\"=\"*50)\n        print(f\"Total Records: {stats['total_records']}\")\n        print(f\"Unique Images: {stats['unique_images']}\")\n        print(f\"Average Confidence: {stats['avg_confidence']:.3f}\")\n        print(f\"Embedding Dimension: {stats['embedding_dimension']}\")\n        \n        if self.records:\n            print(f\"\\nRecent Records:\")\n            for record in self.records[-3:]:  # Show last 3 records\n                filename = record.image_path.split('/')[-1]\n                print(f\"  ID {record.id}: {filename} (conf: {record.confidence:.3f})\")\n        \n        print(\"=\"*50)\n","size_bytes":5566},"face_processor.py":{"content":"\"\"\"\nFace processing module using InsightFace and ArcFace embeddings.\n\"\"\"\n\nimport numpy as np\nimport cv2\nfrom typing import List, Tuple, Optional, Dict\nimport logging\nimport os\nfrom config import (\n    ARCFACE_MODEL_NAME, FORCE_CPU, GPU_DEVICE_ID, \n    MIN_FACE_SIZE, BATCH_SIZE\n)\nfrom utils import check_gpu_availability\n\nlogger = logging.getLogger(__name__)\n\nclass FaceProcessor:\n    \"\"\"Face detection and embedding extraction using InsightFace.\"\"\"\n    \n    def __init__(self):\n        \"\"\"Initialize the face processor.\"\"\"\n        self.app = None\n        self.model_loaded = False\n        self.using_gpu = False\n        self._initialize_model()\n    \n    def _initialize_model(self):\n        \"\"\"Initialize the InsightFace model.\"\"\"\n        try:\n            import insightface\n            \n            # Check GPU availability\n            gpu_available, gpu_info = check_gpu_availability()\n            \n            # Determine which context to use\n            if FORCE_CPU or not gpu_available:\n                ctx_id = -1  # CPU\n                device_info = \"CPU (forced)\" if FORCE_CPU else f\"CPU (fallback: {gpu_info})\"\n                self.using_gpu = False\n            else:\n                ctx_id = GPU_DEVICE_ID  # GPU\n                device_info = f\"GPU {GPU_DEVICE_ID}\"\n                self.using_gpu = True\n            \n            logger.info(f\"Initializing InsightFace model on {device_info}\")\n            \n            # Initialize the face analysis app\n            self.app = insightface.app.FaceAnalysis(\n                name=ARCFACE_MODEL_NAME,\n                providers=['CUDAExecutionProvider', 'CPUExecutionProvider'] if self.using_gpu else ['CPUExecutionProvider']\n            )\n            \n            # Prepare the model with context\n            self.app.prepare(ctx_id=ctx_id, det_size=(640, 640))\n            \n            self.model_loaded = True\n            logger.info(f\"Successfully initialized {ARCFACE_MODEL_NAME} model on {device_info}\")\n            \n        except ImportError as e:\n            logger.error(f\"InsightFace not available: {str(e)}\")\n            raise RuntimeError(\"InsightFace library is required but not installed\")\n        except Exception as e:\n            logger.error(f\"Failed to initialize face model: {str(e)}\")\n            raise RuntimeError(f\"Model initialization failed: {str(e)}\")\n    \n    def detect_faces(self, image: np.ndarray) -> List[Dict]:\n        \"\"\"\n        Detect faces in an image.\n        \n        Args:\n            image: Input image in RGB format\n            \n        Returns:\n            List of face detection results\n        \"\"\"\n        if not self.model_loaded:\n            raise RuntimeError(\"Face model not loaded\")\n        \n        try:\n            # Detect faces\n            faces = self.app.get(image)\n            \n            # Filter faces by minimum size\n            valid_faces = []\n            for face in faces:\n                bbox = face.bbox.astype(int)\n                width = bbox[2] - bbox[0]\n                height = bbox[3] - bbox[1]\n                \n                if width >= MIN_FACE_SIZE and height >= MIN_FACE_SIZE:\n                    face_info = {\n                        'bbox': (bbox[0], bbox[1], width, height),\n                        'confidence': float(face.det_score),\n                        'embedding': face.embedding,\n                        'landmarks': face.kps if hasattr(face, 'kps') else None\n                    }\n                    valid_faces.append(face_info)\n                else:\n                    logger.debug(f\"Filtered out small face: {width}x{height}\")\n            \n            return valid_faces\n            \n        except Exception as e:\n            logger.error(f\"Face detection failed: {str(e)}\")\n            return []\n    \n    def extract_embedding(self, image: np.ndarray) -> Optional[np.ndarray]:\n        \"\"\"\n        Extract face embedding from an image containing a single face.\n        \n        Args:\n            image: Input image in RGB format\n            \n        Returns:\n            Face embedding vector or None if no face found\n        \"\"\"\n        faces = self.detect_faces(image)\n        \n        if not faces:\n            return None\n        \n        if len(faces) > 1:\n            logger.warning(f\"Multiple faces detected, using the largest one\")\n            # Select the face with largest area\n            faces.sort(key=lambda x: x['bbox'][2] * x['bbox'][3], reverse=True)\n        \n        return faces[0]['embedding']\n    \n    def process_image(self, image_path: str) -> List[Dict]:\n        \"\"\"\n        Process a single image and extract face information.\n        \n        Args:\n            image_path: Path to the image file\n            \n        Returns:\n            List of face information dictionaries\n        \"\"\"\n        from utils import load_and_preprocess_image\n        \n        # Load and preprocess image\n        image = load_and_preprocess_image(image_path)\n        if image is None:\n            logger.warning(f\"Could not load image: {image_path}\")\n            return []\n        \n        # Detect faces\n        faces = self.detect_faces(image)\n        \n        # Add image path to each face info\n        for face in faces:\n            face['image_path'] = image_path\n        \n        logger.debug(f\"Processed {image_path}: found {len(faces)} faces\")\n        return faces\n    \n    def process_batch(self, image_paths: List[str]) -> List[Dict]:\n        \"\"\"\n        Process a batch of images efficiently.\n        \n        Args:\n            image_paths: List of image file paths\n            \n        Returns:\n            List of all face information dictionaries\n        \"\"\"\n        all_faces = []\n        \n        logger.info(f\"Processing batch of {len(image_paths)} images\")\n        \n        for i, image_path in enumerate(image_paths):\n            if i % 10 == 0:  # Log progress every 10 images\n                logger.info(f\"Processing image {i+1}/{len(image_paths)}\")\n            \n            faces = self.process_image(image_path)\n            all_faces.extend(faces)\n        \n        logger.info(f\"Batch processing complete: {len(all_faces)} faces found in {len(image_paths)} images\")\n        return all_faces\n    \n    def get_model_info(self) -> Dict:\n        \"\"\"\n        Get information about the loaded model.\n        \n        Returns:\n            Dictionary with model information\n        \"\"\"\n        return {\n            'model_name': ARCFACE_MODEL_NAME,\n            'model_loaded': self.model_loaded,\n            'using_gpu': self.using_gpu,\n            'gpu_device_id': GPU_DEVICE_ID if self.using_gpu else None,\n            'embedding_dimension': 512  # buffalo_l produces 512-dim embeddings\n        }\n    \n    def cleanup(self):\n        \"\"\"Clean up resources.\"\"\"\n        if self.app is not None:\n            # InsightFace doesn't have explicit cleanup, but we can clear the reference\n            self.app = None\n            self.model_loaded = False\n            logger.info(\"Face processor cleaned up\")\n","size_bytes":6979},"main.py":{"content":"#!/usr/bin/env python3\n\"\"\"\nGPU-accelerated Python face recognition script using ArcFace embeddings and FAISS-GPU.\n\nThis script provides batch processing of face images, efficient similarity matching,\nand GPU acceleration with automatic CPU fallback.\n\"\"\"\n\nimport os\nimport sys\nimport argparse\nimport time\nfrom typing import List, Dict, Optional\nimport logging\n\n# Import our modules\nfrom config import (\n    SIMILARITY_THRESHOLD, MAX_IMAGES_TO_PROCESS, SHOW_PROGRESS,\n    EMBEDDING_DIMENSION\n)\nfrom utils import (\n    setup_logging, get_image_files, load_and_preprocess_image,\n    print_system_info, create_progress_bar\n)\nfrom database import FaceDatabase\nfrom face_processor import FaceProcessor\nfrom similarity_matcher import SimilarityMatcher\n\nlogger = logging.getLogger(__name__)\n\nclass FaceRecognitionSystem:\n    \"\"\"Main face recognition system orchestrating all components.\"\"\"\n    \n    def __init__(self):\n        \"\"\"Initialize the face recognition system.\"\"\"\n        self.database = FaceDatabase()\n        self.face_processor = None\n        self.similarity_matcher = None\n        self._initialized = False\n    \n    def initialize(self):\n        \"\"\"Initialize all system components.\"\"\"\n        if self._initialized:\n            return\n        \n        logger.info(\"Initializing Face Recognition System...\")\n        \n        try:\n            # Initialize face processor\n            print(\"🔄 Initializing face detection and embedding extraction...\")\n            self.face_processor = FaceProcessor()\n            \n            # Initialize similarity matcher\n            print(\"🔄 Initializing similarity matching system...\")\n            self.similarity_matcher = SimilarityMatcher(EMBEDDING_DIMENSION)\n            \n            self._initialized = True\n            logger.info(\"System initialization complete\")\n            \n            # Print system information\n            self._print_initialization_summary()\n            \n        except Exception as e:\n            logger.error(f\"System initialization failed: {str(e)}\")\n            raise RuntimeError(f\"Failed to initialize system: {str(e)}\")\n    \n    def _print_initialization_summary(self):\n        \"\"\"Print system initialization summary.\"\"\"\n        face_info = self.face_processor.get_model_info()\n        matcher_info = self.similarity_matcher.get_index_info()\n        \n        print(\"\\n\" + \"=\"*60)\n        print(\"🚀 SYSTEM INITIALIZATION COMPLETE\")\n        print(\"=\"*60)\n        print(f\"Face Model: {face_info['model_name']}\")\n        print(f\"Face Processing: {'GPU' if face_info['using_gpu'] else 'CPU'}\")\n        print(f\"Similarity Matching: {'GPU' if matcher_info['using_gpu'] else 'CPU'}\")\n        print(f\"Embedding Dimension: {face_info['embedding_dimension']}\")\n        print(f\"Similarity Threshold: {SIMILARITY_THRESHOLD}\")\n        print(\"=\"*60)\n    \n    def load_images_from_folder(self, folder_path: str, max_images: int = None) -> int:\n        \"\"\"\n        Load and process all images from a folder.\n        \n        Args:\n            folder_path: Path to folder containing images\n            max_images: Maximum number of images to process\n            \n        Returns:\n            Number of faces added to database\n        \"\"\"\n        if not self._initialized:\n            self.initialize()\n        \n        # Get image files\n        image_files = get_image_files(folder_path)\n        if not image_files:\n            print(f\"❌ No supported image files found in {folder_path}\")\n            return 0\n        \n        # Limit number of images if specified\n        if max_images:\n            image_files = image_files[:max_images]\n        \n        # Limit to maximum allowed\n        if len(image_files) > MAX_IMAGES_TO_PROCESS:\n            logger.warning(f\"Limiting to {MAX_IMAGES_TO_PROCESS} images (found {len(image_files)})\")\n            image_files = image_files[:MAX_IMAGES_TO_PROCESS]\n        \n        print(f\"\\n📁 Processing {len(image_files)} images from {folder_path}\")\n        \n        # Process images\n        total_faces = 0\n        processed_images = 0\n        start_time = time.time()\n        \n        for i, image_path in enumerate(image_files):\n            try:\n                # Show progress\n                if SHOW_PROGRESS and (i % 10 == 0 or i == len(image_files) - 1):\n                    progress = create_progress_bar(i + 1, len(image_files), \"Processing\")\n                    print(f\"\\r{progress}\", end=\"\", flush=True)\n                \n                # Process image\n                faces = self.face_processor.process_image(image_path)\n                \n                # Add faces to database\n                for face in faces:\n                    self.database.add_face(\n                        image_path=face['image_path'],\n                        embedding=face['embedding'],\n                        face_bbox=face['bbox'],\n                        confidence=face['confidence']\n                    )\n                    total_faces += 1\n                \n                processed_images += 1\n                \n            except Exception as e:\n                logger.error(f\"Error processing {image_path}: {str(e)}\")\n                continue\n        \n        if SHOW_PROGRESS:\n            print()  # New line after progress bar\n        \n        # Update similarity matcher with all embeddings\n        if total_faces > 0:\n            print(\"🔄 Building similarity search index...\")\n            embeddings = self.database.get_all_embeddings()\n            self.similarity_matcher.add_embeddings(embeddings)\n        \n        # Print summary\n        elapsed_time = time.time() - start_time\n        print(f\"\\n✅ Processing complete!\")\n        print(f\"   📊 Processed: {processed_images}/{len(image_files)} images\")\n        print(f\"   👥 Found: {total_faces} faces\")\n        print(f\"   ⏱️  Time: {elapsed_time:.2f} seconds\")\n        print(f\"   🚀 Speed: {processed_images/elapsed_time:.1f} images/sec\")\n        \n        return total_faces\n    \n    def find_matches(self, selfie_path: str, max_matches: int = 10, \n                    threshold: float = None) -> List[Dict]:\n        \"\"\"\n        Find matching faces for a selfie image.\n        \n        Args:\n            selfie_path: Path to the selfie image\n            max_matches: Maximum number of matches to return\n            threshold: Similarity threshold (uses config default if None)\n            \n        Returns:\n            List of matching results\n        \"\"\"\n        if not self._initialized:\n            raise RuntimeError(\"System not initialized\")\n        \n        if self.database.size() == 0:\n            print(\"❌ No faces in database. Load images first.\")\n            return []\n        \n        print(f\"\\n🔍 Searching for matches for: {selfie_path}\")\n        \n        # Load and process selfie\n        image = load_and_preprocess_image(selfie_path)\n        if image is None:\n            print(f\"❌ Could not load selfie image: {selfie_path}\")\n            return []\n        \n        # Extract embedding\n        embedding = self.face_processor.extract_embedding(image)\n        if embedding is None:\n            print(f\"❌ No face detected in selfie: {selfie_path}\")\n            return []\n        \n        print(f\"✅ Face detected in selfie\")\n        \n        # Find matches\n        image_paths = self.database.get_image_paths()\n        matches = self.similarity_matcher.find_matches(\n            query_embedding=embedding,\n            image_paths=image_paths,\n            k=max_matches,\n            threshold=threshold or SIMILARITY_THRESHOLD\n        )\n        \n        # Print results\n        if matches:\n            print(f\"\\n🎯 Found {len(matches)} matches:\")\n            print(\"-\" * 80)\n            for i, match in enumerate(matches, 1):\n                filename = os.path.basename(match['image_path'])\n                print(f\"{i:2d}. {filename:<40} (similarity: {match['similarity']:.3f})\")\n            print(\"-\" * 80)\n        else:\n            print(f\"❌ No matches found above threshold {threshold or SIMILARITY_THRESHOLD}\")\n        \n        return matches\n    \n    def print_database_summary(self):\n        \"\"\"Print database summary.\"\"\"\n        self.database.print_summary()\n    \n    def cleanup(self):\n        \"\"\"Clean up system resources.\"\"\"\n        if self.face_processor:\n            self.face_processor.cleanup()\n        if self.similarity_matcher:\n            self.similarity_matcher.cleanup()\n        logger.info(\"System cleanup complete\")\n\ndef main():\n    \"\"\"Main function.\"\"\"\n    # Setup logging\n    setup_logging()\n    \n    # Parse command line arguments\n    parser = argparse.ArgumentParser(\n        description=\"GPU-accelerated face recognition using ArcFace and FAISS\",\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        epilog=\"\"\"\nExamples:\n  python main.py --folder ./photos --selfie ./my_selfie.jpg\n  python main.py --folder ./event_photos --selfie ./query.jpg --max-matches 20\n  python main.py --folder ./images --selfie ./face.jpg --threshold 0.7\n        \"\"\"\n    )\n    \n    parser.add_argument('--folder', type=str,\n                       help='Folder containing images to process')\n    parser.add_argument('--selfie', type=str,\n                       help='Path to selfie image for matching')\n    parser.add_argument('--max-images', type=int, default=None,\n                       help='Maximum number of images to process from folder')\n    parser.add_argument('--max-matches', type=int, default=10,\n                       help='Maximum number of matches to return')\n    parser.add_argument('--threshold', type=float, default=None,\n                       help=f'Similarity threshold (default: {SIMILARITY_THRESHOLD})')\n    parser.add_argument('--system-info', action='store_true',\n                       help='Print system information and exit')\n    \n    args = parser.parse_args()\n    \n    # Print system info if requested\n    if args.system_info:\n        print_system_info()\n        return\n    \n    # Validate required inputs for normal operation\n    if not args.folder or not args.selfie:\n        parser.error(\"--folder and --selfie are required unless using --system-info\")\n    \n    # Validate inputs exist\n    if not os.path.exists(args.folder):\n        print(f\"❌ Folder does not exist: {args.folder}\")\n        sys.exit(1)\n    \n    if not os.path.exists(args.selfie):\n        print(f\"❌ Selfie file does not exist: {args.selfie}\")\n        sys.exit(1)\n    \n    # Initialize system\n    system = FaceRecognitionSystem()\n    \n    try:\n        # Print welcome message\n        print(\"🎭 GPU-Accelerated Face Recognition System\")\n        print(\"   Using ArcFace embeddings and FAISS similarity search\")\n        print()\n        \n        # Load and process images\n        faces_found = system.load_images_from_folder(\n            args.folder, \n            max_images=args.max_images\n        )\n        \n        if faces_found == 0:\n            print(\"❌ No faces found in the provided images\")\n            sys.exit(1)\n        \n        # Print database summary\n        system.print_database_summary()\n        \n        # Find matches for selfie\n        matches = system.find_matches(\n            args.selfie,\n            max_matches=args.max_matches,\n            threshold=args.threshold\n        )\n        \n        # Print final summary\n        print(f\"\\n🏁 Search complete: {len(matches)} matches found\")\n        \n    except KeyboardInterrupt:\n        print(\"\\n⚠️  Operation cancelled by user\")\n        sys.exit(1)\n    except Exception as e:\n        logger.error(f\"System error: {str(e)}\")\n        print(f\"❌ Error: {str(e)}\")\n        sys.exit(1)\n    finally:\n        system.cleanup()\n\nif __name__ == \"__main__\":\n    main()\n","size_bytes":11664},"pyproject.toml":{"content":"[project]\nname = \"repl-nix-workspace\"\nversion = \"0.1.0\"\ndescription = \"Add your description here\"\nrequires-python = \">=3.11\"\ndependencies = [\n    \"faiss-cpu>=1.11.0\",\n    \"flask>=3.1.1\",\n    \"google-api-python-client>=2.183.0\",\n    \"google-auth>=2.41.1\",\n    \"google-auth-oauthlib>=1.2.2\",\n    \"insightface>=0.7.3\",\n    \"numpy>=2.3.1\",\n    \"onnxruntime>=1.22.0\",\n    \"opencv-python>=4.11.0.86\",\n    \"requests>=2.32.4\",\n    \"werkzeug>=3.1.3\",\n]\n","size_bytes":444},"replit.md":{"content":"# Face Recognition System\n\n## Overview\n\nThis is a GPU-accelerated Python face recognition system built using ArcFace embeddings and FAISS for efficient similarity matching. The system provides batch processing of face images with automatic GPU acceleration and CPU fallback capabilities. It uses InsightFace for face detection and embedding extraction, storing face data in an in-memory database for fast retrieval.\n\n## System Architecture\n\nThe application follows a modular architecture with clear separation of concerns:\n\n- **Configuration Layer**: Centralized configuration management through `config.py`\n- **Processing Layer**: Face detection and embedding extraction via `face_processor.py`\n- **Storage Layer**: In-memory database for face records via `database.py`\n- **Matching Layer**: FAISS-based similarity matching through `similarity_matcher.py`\n- **Orchestration Layer**: Main system coordination via `main.py`\n- **Utility Layer**: Common utilities and helper functions in `utils.py`\n\nThe architecture prioritizes performance through GPU acceleration while maintaining robustness with automatic CPU fallback mechanisms.\n\n## Key Components\n\n### Face Processor (`face_processor.py`)\n- **Purpose**: Face detection and embedding extraction using InsightFace\n- **Technology**: InsightFace library with ArcFace model (buffalo_l)\n- **Features**: GPU acceleration with automatic CPU fallback, batch processing support\n- **Output**: 512-dimensional face embeddings with bounding box coordinates\n\n### In-Memory Database (`database.py`)\n- **Purpose**: Store face embeddings and metadata for fast retrieval\n- **Architecture**: Simple in-memory storage using Python lists and NumPy arrays\n- **Data Structure**: FaceRecord dataclass containing embeddings, bounding boxes, confidence scores, and timestamps\n- **Features**: Automatic embedding matrix rebuilding, efficient batch operations\n\n### Similarity Matcher (`similarity_matcher.py`)\n- **Purpose**: Efficient similarity search using FAISS\n- **Technology**: FAISS library with GPU support\n- **Features**: GPU-accelerated vector search with CPU fallback, configurable similarity thresholds\n- **Performance**: Optimized for large-scale face matching operations\n\n### Configuration Management (`config.py`)\n- **Purpose**: Centralized system configuration\n- **Features**: Environment variable support, GPU/CPU device selection, processing parameters\n- **Flexibility**: Easy tuning of similarity thresholds, batch sizes, and model parameters\n\n## Data Flow\n\n1. **Image Input**: System accepts folder paths containing images in supported formats\n2. **Preprocessing**: Images are loaded, resized, and prepared for face detection\n3. **Face Detection**: InsightFace processes images to detect faces and extract embeddings\n4. **Storage**: Face records are stored in the in-memory database with metadata\n5. **Indexing**: FAISS index is built/updated for efficient similarity search\n6. **Matching**: Query embeddings are compared against the database using FAISS\n7. **Results**: Similar faces are returned with confidence scores and metadata\n\n## External Dependencies\n\n### Core Libraries\n- **InsightFace**: Face detection and ArcFace embedding extraction\n- **FAISS**: Efficient similarity search and clustering\n- **OpenCV**: Image processing and manipulation\n- **NumPy**: Numerical operations and array handling\n\n### GPU Dependencies\n- **CUDA**: Required for GPU acceleration (optional, CPU fallback available)\n- **cuDNN**: Deep learning primitives for GPU acceleration\n\n### Python Packages\n- Standard library modules: `os`, `sys`, `logging`, `datetime`, `dataclasses`\n- Image processing: `cv2` (OpenCV)\n- Scientific computing: `numpy`\n\n## Deployment Strategy\n\n### Environment Setup\n- Python 3.7+ required\n- GPU support optional but recommended for performance\n- Automatic dependency detection and fallback mechanisms\n\n### Hardware Requirements\n- **Minimum**: CPU-only operation with 4GB RAM\n- **Recommended**: NVIDIA GPU with CUDA support, 8GB+ RAM\n- **Storage**: Minimal disk usage (in-memory database)\n\n### Configuration Options\n- Environment variables for GPU device selection\n- Configurable batch sizes and processing parameters\n- Flexible similarity thresholds and model selection\n\n## Google Drive Integration\n\nThe system now includes integration with Google Drive for cloud-based face matching:\n\n- **Google Drive Folder**: Configured to access folder ID `1eoM3z7eWgGmLsFM9rhuL17zsB8Cd6yUq`\n- **Automatic Matching**: When a photo is uploaded, the system automatically compares it with images stored in the configured Google Drive folder\n- **Face Comparison**: Uses the same ArcFace embeddings and cosine similarity (threshold 0.6) to find matching faces\n- **Performance**: Processes up to 50 images from the Drive folder per request to maintain reasonable response times\n- **Visual Results**: Displays matched images as thumbnails with similarity percentages in the UI\n- **Image Serving**: Provides a route to serve images directly from Google Drive for display\n\n### Setup Requirements\n- Google Drive must be connected through Replit's integration panel\n- The connected Google account must have access to the configured folder\n\n## Changelog\n\n```\nChangelog:\n- July 01, 2025. Initial setup - Created modular face recognition system\n- July 01, 2025. Added GPU acceleration with CPU fallback\n- July 01, 2025. Implemented FAISS similarity matching\n- July 01, 2025. Added comprehensive CLI interface\n- July 01, 2025. Created working face recognition pipeline\n- October 01, 2025. Added Google Drive integration for cloud-based face matching\n- October 01, 2025. Implemented thumbnail display of matched images from Drive\n```\n\n## User Preferences\n\n```\nPreferred communication style: Simple, everyday language.\n```","size_bytes":5747},"similarity_matcher.py":{"content":"\"\"\"\nSimilarity matching using FAISS for efficient vector search.\n\"\"\"\n\nimport numpy as np\nfrom typing import List, Tuple, Optional, Dict\nimport logging\nfrom config import SIMILARITY_THRESHOLD, FORCE_CPU, GPU_DEVICE_ID\nfrom utils import check_gpu_availability\n\nlogger = logging.getLogger(__name__)\n\nclass SimilarityMatcher:\n    \"\"\"FAISS-based similarity matcher for face embeddings.\"\"\"\n    \n    def __init__(self, embedding_dimension: int = 512):\n        \"\"\"\n        Initialize the similarity matcher.\n        \n        Args:\n            embedding_dimension: Dimension of the embedding vectors\n        \"\"\"\n        self.embedding_dimension = embedding_dimension\n        self.index = None\n        self.using_gpu = False\n        self.gpu_resources = None\n        self._initialize_faiss()\n    \n    def _initialize_faiss(self):\n        \"\"\"Initialize FAISS index with GPU support if available.\"\"\"\n        try:\n            import faiss\n            \n            # Check GPU availability and FAISS GPU support\n            gpu_available, gpu_info = check_gpu_availability()\n            has_gpu_support = hasattr(faiss, 'StandardGpuResources')\n            \n            # Determine which device to use\n            if FORCE_CPU or not gpu_available or not has_gpu_support:\n                self._initialize_cpu_index(faiss)\n                if FORCE_CPU:\n                    device_info = \"CPU (forced)\"\n                elif not gpu_available:\n                    device_info = f\"CPU (no GPU: {gpu_info})\"\n                else:\n                    device_info = \"CPU (FAISS-GPU not available)\"\n            else:\n                try:\n                    self._initialize_gpu_index(faiss)\n                    device_info = f\"GPU {GPU_DEVICE_ID}\"\n                except Exception as e:\n                    logger.warning(f\"GPU initialization failed, falling back to CPU: {str(e)}\")\n                    self._initialize_cpu_index(faiss)\n                    device_info = f\"CPU (GPU fallback: {str(e)})\"\n            \n            logger.info(f\"Initialized FAISS index on {device_info}\")\n            \n        except ImportError as e:\n            logger.error(f\"FAISS not available: {str(e)}\")\n            raise RuntimeError(\"FAISS library is required but not installed\")\n    \n    def _initialize_cpu_index(self, faiss):\n        \"\"\"Initialize CPU-based FAISS index.\"\"\"\n        # Use inner product for cosine similarity (normalized vectors)\n        self.index = faiss.IndexFlatIP(self.embedding_dimension)\n        self.using_gpu = False\n        logger.debug(\"Initialized CPU FAISS index (IndexFlatIP)\")\n    \n    def _initialize_gpu_index(self, faiss):\n        \"\"\"Initialize GPU-based FAISS index.\"\"\"\n        # Create GPU resources\n        self.gpu_resources = faiss.StandardGpuResources()\n        \n        # Create CPU index first\n        cpu_index = faiss.IndexFlatIP(self.embedding_dimension)\n        \n        # Move to GPU\n        self.index = faiss.index_cpu_to_gpu(\n            self.gpu_resources, \n            GPU_DEVICE_ID, \n            cpu_index\n        )\n        \n        self.using_gpu = True\n        logger.debug(f\"Initialized GPU FAISS index on device {GPU_DEVICE_ID}\")\n    \n    def add_embeddings(self, embeddings: np.ndarray):\n        \"\"\"\n        Add embeddings to the FAISS index.\n        \n        Args:\n            embeddings: Matrix of embeddings (n_vectors x embedding_dim)\n        \"\"\"\n        if embeddings.size == 0:\n            logger.warning(\"Attempted to add empty embeddings array\")\n            return\n        \n        if embeddings.shape[1] != self.embedding_dimension:\n            raise ValueError(f\"Embedding dimension mismatch: expected {self.embedding_dimension}, got {embeddings.shape[1]}\")\n        \n        # Normalize embeddings for cosine similarity\n        normalized_embeddings = self._normalize_embeddings(embeddings)\n        \n        # Add to index\n        self.index.add(normalized_embeddings.astype(np.float32))\n        \n        logger.debug(f\"Added {embeddings.shape[0]} embeddings to FAISS index\")\n    \n    def search(self, query_embedding: np.ndarray, k: int = 10, \n               threshold: float = None) -> Tuple[List[float], List[int]]:\n        \"\"\"\n        Search for similar embeddings.\n        \n        Args:\n            query_embedding: Query embedding vector\n            k: Number of nearest neighbors to return\n            threshold: Similarity threshold (uses config default if None)\n            \n        Returns:\n            Tuple of (similarities, indices)\n        \"\"\"\n        if self.index is None:\n            raise RuntimeError(\"FAISS index not initialized\")\n        \n        if self.index.ntotal == 0:\n            logger.warning(\"FAISS index is empty\")\n            return [], []\n        \n        if threshold is None:\n            threshold = SIMILARITY_THRESHOLD\n        \n        # Normalize query embedding\n        query_normalized = self._normalize_embeddings(query_embedding.reshape(1, -1))\n        \n        # Search\n        similarities, indices = self.index.search(\n            query_normalized.astype(np.float32), \n            min(k, self.index.ntotal)\n        )\n        \n        # Filter by threshold and convert to lists\n        valid_similarities = []\n        valid_indices = []\n        \n        for sim, idx in zip(similarities[0], indices[0]):\n            if sim >= threshold and idx != -1:  # -1 indicates invalid result\n                valid_similarities.append(float(sim))\n                valid_indices.append(int(idx))\n        \n        logger.debug(f\"Found {len(valid_similarities)} matches above threshold {threshold}\")\n        return valid_similarities, valid_indices\n    \n    def find_matches(self, query_embedding: np.ndarray, image_paths: List[str],\n                    k: int = 10, threshold: float = None) -> List[Dict]:\n        \"\"\"\n        Find matching images for a query embedding.\n        \n        Args:\n            query_embedding: Query embedding vector\n            image_paths: List of image paths corresponding to stored embeddings\n            k: Maximum number of matches to return\n            threshold: Similarity threshold\n            \n        Returns:\n            List of match dictionaries with similarity scores and paths\n        \"\"\"\n        similarities, indices = self.search(query_embedding, k, threshold)\n        \n        matches = []\n        for sim, idx in zip(similarities, indices):\n            if idx < len(image_paths):\n                matches.append({\n                    'image_path': image_paths[idx],\n                    'similarity': sim,\n                    'index': idx\n                })\n        \n        # Sort by similarity (highest first)\n        matches.sort(key=lambda x: x['similarity'], reverse=True)\n        \n        return matches\n    \n    def _normalize_embeddings(self, embeddings: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Normalize embeddings for cosine similarity.\n        \n        Args:\n            embeddings: Input embeddings\n            \n        Returns:\n            Normalized embeddings\n        \"\"\"\n        norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n        # Avoid division by zero\n        norms = np.maximum(norms, 1e-8)\n        return embeddings / norms\n    \n    def get_index_info(self) -> Dict:\n        \"\"\"\n        Get information about the FAISS index.\n        \n        Returns:\n            Dictionary with index information\n        \"\"\"\n        return {\n            'embedding_dimension': self.embedding_dimension,\n            'using_gpu': self.using_gpu,\n            'gpu_device_id': GPU_DEVICE_ID if self.using_gpu else None,\n            'total_vectors': self.index.ntotal if self.index else 0,\n            'index_type': type(self.index).__name__ if self.index else None\n        }\n    \n    def clear(self):\n        \"\"\"Clear the FAISS index.\"\"\"\n        if self.index is not None:\n            self.index.reset()\n            logger.debug(\"Cleared FAISS index\")\n    \n    def cleanup(self):\n        \"\"\"Clean up GPU resources.\"\"\"\n        if self.using_gpu and self.gpu_resources is not None:\n            # FAISS GPU resources are automatically managed\n            self.gpu_resources = None\n            logger.debug(\"Cleaned up GPU resources\")\n","size_bytes":8170},"simple_example.py":{"content":"#!/usr/bin/env python3\n\"\"\"\nSimple example demonstrating the face recognition system.\n\"\"\"\n\nimport os\nimport sys\nimport cv2\nimport numpy as np\n\n# Add the current directory to Python path\nsys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))\n\nfrom main import FaceRecognitionSystem\n\ndef create_realistic_face_pattern(filename, variation=0):\n    \"\"\"Create a simple pattern that might be detected as a face.\"\"\"\n    # Create a 300x300 image\n    img = np.ones((300, 300, 3), dtype=np.uint8) * 240\n    \n    # Add noise for more realistic texture\n    noise = np.random.normal(0, 10, img.shape).astype(np.uint8)\n    img = cv2.add(img, noise)\n    \n    # Create face-like pattern with gradients\n    center_x, center_y = 150, 150\n    \n    # Face outline (larger oval)\n    cv2.ellipse(img, (center_x, center_y), (80, 100), 0, 0, 360, (200+variation, 180, 160), -1)\n    \n    # Eyes (dark circles)\n    cv2.circle(img, (center_x-25, center_y-20), 8, (50, 50, 50), -1)\n    cv2.circle(img, (center_x+25, center_y-20), 8, (50, 50, 50), -1)\n    \n    # Eye highlights\n    cv2.circle(img, (center_x-23, center_y-22), 3, (255, 255, 255), -1)\n    cv2.circle(img, (center_x+23, center_y-22), 3, (255, 255, 255), -1)\n    \n    # Nose\n    pts = np.array([[center_x, center_y-5], [center_x-5, center_y+10], [center_x+5, center_y+10]], np.int32)\n    cv2.fillPoly(img, [pts], (150, 120, 100))\n    \n    # Mouth\n    cv2.ellipse(img, (center_x, center_y+25), (15, 8), 0, 0, 180, (100, 80, 80), 2)\n    \n    # Add some shading for depth\n    overlay = img.copy()\n    cv2.ellipse(overlay, (center_x-15, center_y-15), (60, 80), 0, 0, 360, (180, 150, 130), -1)\n    img = cv2.addWeighted(img, 0.7, overlay, 0.3, 0)\n    \n    cv2.imwrite(filename, img)\n    print(f\"Created face pattern: {filename}\")\n\ndef run_example():\n    \"\"\"Run the face recognition example.\"\"\"\n    print(\"Creating Face Recognition System Example\")\n    print(\"=\" * 50)\n    \n    # Create test images\n    os.makedirs('demo_images', exist_ok=True)\n    \n    create_realistic_face_pattern('demo_images/person1.jpg', 0)\n    create_realistic_face_pattern('demo_images/person2.jpg', 20)\n    create_realistic_face_pattern('demo_images/person3.jpg', -15)\n    create_realistic_face_pattern('query_face.jpg', 5)  # Similar to person1\n    \n    print(f\"\\nCreated demo images:\")\n    print(f\"- demo_images/person1.jpg\")\n    print(f\"- demo_images/person2.jpg\") \n    print(f\"- demo_images/person3.jpg\")\n    print(f\"- query_face.jpg (for matching)\")\n    \n    print(f\"\\nInitializing Face Recognition System...\")\n    \n    try:\n        # Initialize system\n        system = FaceRecognitionSystem()\n        system.initialize()\n        \n        # Load images from folder\n        print(f\"\\nLoading images from demo_images folder...\")\n        faces_found = system.load_images_from_folder('demo_images', max_images=10)\n        \n        if faces_found > 0:\n            # Show database summary\n            system.print_database_summary()\n            \n            # Find matches for query\n            print(f\"\\nSearching for matches...\")\n            matches = system.find_matches('query_face.jpg', max_matches=5, threshold=0.3)\n            \n            print(f\"\\nExample completed successfully!\")\n            print(f\"Found {len(matches)} potential matches\")\n            \n        else:\n            print(\"No faces detected in the demo images\")\n            print(\"Note: The system uses real face detection - simple patterns may not be detected\")\n        \n        # Cleanup\n        system.cleanup()\n        \n    except Exception as e:\n        print(f\"Error during example: {e}\")\n        import traceback\n        traceback.print_exc()\n\nif __name__ == \"__main__\":\n    run_example()","size_bytes":3679},"test_demo.py":{"content":"#!/usr/bin/env python3\n\"\"\"\nDemo script for the face recognition system.\nThis creates sample images for testing the system.\n\"\"\"\n\nimport cv2\nimport numpy as np\nimport os\nfrom main import FaceRecognitionSystem\n\ndef create_sample_face_image(filename, color=(100, 150, 200)):\n    \"\"\"Create a simple sample face image for testing.\"\"\"\n    # Create a simple face-like image with basic features\n    img = np.ones((200, 200, 3), dtype=np.uint8) * 255\n    \n    # Add a simple face shape (oval)\n    center = (100, 100)\n    axes = (80, 100)\n    cv2.ellipse(img, center, axes, 0, 0, 360, color, -1)\n    \n    # Add eyes\n    cv2.circle(img, (80, 80), 10, (0, 0, 0), -1)\n    cv2.circle(img, (120, 80), 10, (0, 0, 0), -1)\n    \n    # Add nose\n    cv2.circle(img, (100, 100), 5, (50, 50, 50), -1)\n    \n    # Add mouth\n    cv2.ellipse(img, (100, 130), (20, 10), 0, 0, 180, (0, 0, 0), 2)\n    \n    # Save the image\n    cv2.imwrite(filename, img)\n    print(f\"Created sample face image: {filename}\")\n\ndef demo_face_recognition():\n    \"\"\"Demonstrate the face recognition system.\"\"\"\n    # Create test images directory\n    os.makedirs('test_images', exist_ok=True)\n    \n    # Create sample face images with different colors\n    create_sample_face_image('test_images/face1.jpg', (120, 160, 200))\n    create_sample_face_image('test_images/face2.jpg', (100, 140, 180))\n    create_sample_face_image('test_images/face3.jpg', (140, 170, 220))\n    create_sample_face_image('selfie.jpg', (120, 160, 200))  # Similar to face1\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"FACE RECOGNITION DEMO\")\n    print(\"=\"*60)\n    print(\"Created sample images for testing:\")\n    print(\"- test_images/face1.jpg\")\n    print(\"- test_images/face2.jpg\") \n    print(\"- test_images/face3.jpg\")\n    print(\"- selfie.jpg (similar to face1)\")\n    print(\"\\nNow running face recognition system...\")\n    print(\"=\"*60)\n\nif __name__ == \"__main__\":\n    demo_face_recognition()","size_bytes":1904},"utils.py":{"content":"\"\"\"\nUtility functions for the face recognition system.\n\"\"\"\n\nimport os\nimport cv2\nimport numpy as np\nfrom typing import List, Tuple, Optional\nimport logging\nfrom config import SUPPORTED_IMAGE_FORMATS, MAX_IMAGE_SIZE, LOG_LEVEL\n\n# Configure logging\nlogging.basicConfig(level=getattr(logging, LOG_LEVEL))\nlogger = logging.getLogger(__name__)\n\ndef setup_logging():\n    \"\"\"Setup logging configuration.\"\"\"\n    logging.basicConfig(\n        level=getattr(logging, LOG_LEVEL),\n        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n        datefmt='%Y-%m-%d %H:%M:%S'\n    )\n\ndef get_image_files(folder_path: str) -> List[str]:\n    \"\"\"\n    Get all supported image files from a folder.\n    \n    Args:\n        folder_path: Path to the folder containing images\n        \n    Returns:\n        List of image file paths\n    \"\"\"\n    if not os.path.exists(folder_path):\n        logger.error(f\"Folder does not exist: {folder_path}\")\n        return []\n    \n    image_files = []\n    for root, dirs, files in os.walk(folder_path):\n        for file in files:\n            if any(file.lower().endswith(ext) for ext in SUPPORTED_IMAGE_FORMATS):\n                image_files.append(os.path.join(root, file))\n    \n    logger.info(f\"Found {len(image_files)} image files in {folder_path}\")\n    return image_files\n\ndef load_and_preprocess_image(image_path: str) -> Optional[np.ndarray]:\n    \"\"\"\n    Load and preprocess an image for face detection.\n    \n    Args:\n        image_path: Path to the image file\n        \n    Returns:\n        Preprocessed image array or None if failed\n    \"\"\"\n    try:\n        # Load image\n        image = cv2.imread(image_path)\n        if image is None:\n            logger.warning(f\"Could not load image: {image_path}\")\n            return None\n        \n        # Convert BGR to RGB\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        # Resize if too large\n        height, width = image.shape[:2]\n        if max(height, width) > MAX_IMAGE_SIZE:\n            scale = MAX_IMAGE_SIZE / max(height, width)\n            new_width = int(width * scale)\n            new_height = int(height * scale)\n            image = cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_AREA)\n            logger.debug(f\"Resized image {image_path} from {width}x{height} to {new_width}x{new_height}\")\n        \n        return image\n    \n    except Exception as e:\n        logger.error(f\"Error loading image {image_path}: {str(e)}\")\n        return None\n\ndef check_gpu_availability() -> Tuple[bool, str]:\n    \"\"\"\n    Check if GPU is available for processing.\n    \n    Returns:\n        Tuple of (is_available, device_info)\n    \"\"\"\n    try:\n        import onnxruntime as ort\n        \n        # Check available providers\n        providers = ort.get_available_providers()\n        \n        if 'CUDAExecutionProvider' in providers:\n            # Try to create a session to verify CUDA works\n            try:\n                session = ort.InferenceSession(\n                    providers=['CUDAExecutionProvider'],\n                    sess_options=ort.SessionOptions()\n                )\n                return True, \"CUDA GPU detected and available\"\n            except Exception as e:\n                logger.warning(f\"CUDA provider available but failed to initialize: {str(e)}\")\n                return False, f\"CUDA initialization failed: {str(e)}\"\n        else:\n            return False, \"CUDA provider not available\"\n    \n    except ImportError:\n        return False, \"ONNXRuntime not available\"\n    except Exception as e:\n        return False, f\"GPU check failed: {str(e)}\"\n\ndef print_system_info():\n    \"\"\"Print system information for debugging.\"\"\"\n    gpu_available, gpu_info = check_gpu_availability()\n    \n    print(\"=\"*60)\n    print(\"FACE RECOGNITION SYSTEM - SYSTEM INFO\")\n    print(\"=\"*60)\n    print(f\"GPU Available: {gpu_available}\")\n    print(f\"GPU Info: {gpu_info}\")\n    \n    try:\n        import faiss\n        print(f\"FAISS Version: {faiss.__version__}\")\n        print(f\"FAISS GPU Support: {hasattr(faiss, 'StandardGpuResources')}\")\n    except ImportError:\n        print(\"FAISS not available\")\n    \n    try:\n        import insightface\n        print(f\"InsightFace Available: True\")\n    except ImportError:\n        print(\"InsightFace not available\")\n    \n    print(\"=\"*60)\n\ndef create_progress_bar(current: int, total: int, prefix: str = \"Progress\") -> str:\n    \"\"\"\n    Create a simple text progress bar.\n    \n    Args:\n        current: Current progress value\n        total: Total progress value\n        prefix: Prefix text\n        \n    Returns:\n        Progress bar string\n    \"\"\"\n    if total == 0:\n        return f\"{prefix}: 0/0 (100%)\"\n    \n    percentage = (current / total) * 100\n    filled_length = int(50 * current // total)\n    bar = '█' * filled_length + '-' * (50 - filled_length)\n    return f\"{prefix}: |{bar}| {current}/{total} ({percentage:.1f}%)\"\n","size_bytes":4881},"embeddings_store.py":{"content":"\"\"\"\nStorage system for face embeddings.\n\"\"\"\nimport os\nimport json\nimport numpy as np\nfrom typing import List, Dict, Optional, Tuple\nfrom datetime import datetime\n\nclass EmbeddingsStore:\n    \"\"\"Store and retrieve face embeddings.\"\"\"\n    \n    def __init__(self, storage_path: str = 'embeddings_data'):\n        \"\"\"\n        Initialize the embeddings store.\n        \n        Args:\n            storage_path: Directory to store embeddings\n        \"\"\"\n        self.storage_path = storage_path\n        self.embeddings_file = os.path.join(storage_path, 'embeddings.json')\n        self.embeddings_npy = os.path.join(storage_path, 'embeddings.npy')\n        \n        os.makedirs(storage_path, exist_ok=True)\n        \n        self.persons = []\n        self.embeddings = None\n        self._load_data()\n    \n    def _load_data(self):\n        \"\"\"Load embeddings from disk.\"\"\"\n        try:\n            if os.path.exists(self.embeddings_file):\n                with open(self.embeddings_file, 'r') as f:\n                    self.persons = json.load(f)\n                \n                if os.path.exists(self.embeddings_npy):\n                    self.embeddings = np.load(self.embeddings_npy)\n                    print(f\"Loaded {len(self.persons)} person records\")\n                else:\n                    self.embeddings = np.array([])\n            else:\n                print(\"No existing embeddings found, starting fresh\")\n        except Exception as e:\n            print(f\"Error loading embeddings: {str(e)}\")\n            self.persons = []\n            self.embeddings = np.array([])\n    \n    def _save_data(self):\n        \"\"\"Save embeddings to disk.\"\"\"\n        try:\n            with open(self.embeddings_file, 'w') as f:\n                json.dump(self.persons, f, indent=2)\n            \n            if self.embeddings is not None and len(self.embeddings) > 0:\n                np.save(self.embeddings_npy, self.embeddings)\n            \n            print(f\"Saved {len(self.persons)} person records\")\n        except Exception as e:\n            print(f\"Error saving embeddings: {str(e)}\")\n    \n    def add_person(self, name: str, embedding: np.ndarray, photo_url: str = None, \n                   drive_file_id: str = None) -> int:\n        \"\"\"\n        Add a new person with their face embedding.\n        \n        Args:\n            name: Person's name or identifier\n            embedding: Face embedding vector\n            photo_url: URL or path to the photo\n            drive_file_id: Google Drive file ID\n            \n        Returns:\n            Person ID\n        \"\"\"\n        person_id = len(self.persons) + 1\n        \n        person_data = {\n            'id': person_id,\n            'name': name,\n            'photo_url': photo_url,\n            'drive_file_id': drive_file_id,\n            'added_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n        }\n        \n        self.persons.append(person_data)\n        \n        # Add embedding to array\n        if self.embeddings is None or len(self.embeddings) == 0:\n            self.embeddings = embedding.reshape(1, -1)\n        else:\n            self.embeddings = np.vstack([self.embeddings, embedding])\n        \n        self._save_data()\n        print(f\"Added person: {name} (ID: {person_id})\")\n        return person_id\n    \n    def find_match(self, query_embedding: np.ndarray, threshold: float = 0.6) -> Tuple[bool, float, Optional[Dict]]:\n        \"\"\"\n        Find a matching person for the query embedding.\n        \n        Args:\n            query_embedding: Query face embedding\n            threshold: Similarity threshold (0-1, higher = more similar)\n            \n        Returns:\n            Tuple of (found, similarity_score, person_data)\n        \"\"\"\n        if self.embeddings is None or len(self.embeddings) == 0:\n            return False, 0.0, None\n        \n        # Normalize embeddings\n        query_norm = query_embedding / np.linalg.norm(query_embedding)\n        embeddings_norm = self.embeddings / np.linalg.norm(self.embeddings, axis=1, keepdims=True)\n        \n        # Compute cosine similarity\n        similarities = np.dot(embeddings_norm, query_norm)\n        \n        # Find best match\n        best_idx = np.argmax(similarities)\n        best_score = float(similarities[best_idx])\n        \n        if best_score >= threshold:\n            return True, best_score, self.persons[best_idx]\n        else:\n            return False, best_score, None\n    \n    def get_all_persons(self) -> List[Dict]:\n        \"\"\"Get all stored persons.\"\"\"\n        return self.persons\n    \n    def get_person_count(self) -> int:\n        \"\"\"Get the number of stored persons.\"\"\"\n        return len(self.persons)\n    \n    def delete_person(self, person_id: int) -> bool:\n        \"\"\"\n        Delete a person by ID.\n        \n        Args:\n            person_id: Person ID to delete\n            \n        Returns:\n            True if successful\n        \"\"\"\n        try:\n            idx = None\n            for i, person in enumerate(self.persons):\n                if person['id'] == person_id:\n                    idx = i\n                    break\n            \n            if idx is not None:\n                self.persons.pop(idx)\n                if self.embeddings is not None and len(self.embeddings) > 0:\n                    self.embeddings = np.delete(self.embeddings, idx, axis=0)\n                self._save_data()\n                print(f\"Deleted person ID: {person_id}\")\n                return True\n            return False\n        except Exception as e:\n            print(f\"Error deleting person: {str(e)}\")\n            return False\n","size_bytes":5559},"google_drive_helper.py":{"content":"\"\"\"\nGoogle Drive helper for uploading and downloading files.\n\"\"\"\nimport os\nimport io\nimport requests\nfrom typing import Optional\nfrom googleapiclient.discovery import build\nfrom googleapiclient.http import MediaFileUpload, MediaIoBaseDownload\nfrom google.oauth2.credentials import Credentials\n\ndef get_drive_service():\n    \"\"\"Get authenticated Google Drive service.\"\"\"\n    try:\n        # Get connection settings from Replit environment\n        hostname = os.environ.get('REPLIT_CONNECTORS_HOSTNAME')\n        repl_identity = os.environ.get('REPL_IDENTITY')\n        web_repl_renewal = os.environ.get('WEB_REPL_RENEWAL')\n        \n        x_replit_token = None\n        if repl_identity:\n            x_replit_token = f'repl {repl_identity}'\n        elif web_repl_renewal:\n            x_replit_token = f'depl {web_repl_renewal}'\n        \n        if not x_replit_token or not hostname:\n            raise Exception('Replit authentication not available')\n        \n        # Fetch connection settings\n        response = requests.get(\n            f'https://{hostname}/api/v2/connection?include_secrets=true&connector_names=google-drive',\n            headers={\n                'Accept': 'application/json',\n                'X_REPLIT_TOKEN': x_replit_token\n            }\n        )\n        \n        if response.status_code != 200:\n            raise Exception(f'Failed to get connection settings: {response.status_code}')\n        \n        data = response.json()\n        items = data.get('items', [])\n        \n        if not items:\n            raise Exception('Google Drive not connected')\n        \n        connection_settings = items[0]\n        access_token = connection_settings.get('settings', {}).get('access_token')\n        \n        if not access_token:\n            raise Exception('No access token available')\n        \n        # Create credentials\n        credentials = Credentials(token=access_token)\n        \n        # Build Drive service\n        service = build('drive', 'v3', credentials=credentials)\n        return service\n        \n    except Exception as e:\n        print(f\"Error getting Drive service: {str(e)}\")\n        raise\n\ndef upload_file_to_drive(file_path: str, filename: str, folder_id: Optional[str] = None) -> str:\n    \"\"\"\n    Upload a file to Google Drive.\n    \n    Args:\n        file_path: Local path to the file\n        filename: Name for the file in Drive\n        folder_id: Optional folder ID to upload to\n        \n    Returns:\n        File ID of the uploaded file\n    \"\"\"\n    try:\n        service = get_drive_service()\n        \n        file_metadata = {'name': filename}\n        if folder_id:\n            file_metadata['parents'] = [folder_id]\n        \n        media = MediaFileUpload(file_path, resumable=True)\n        \n        file = service.files().create(\n            body=file_metadata,\n            media_body=media,\n            fields='id'\n        ).execute()\n        \n        file_id = file.get('id')\n        print(f\"Uploaded file to Drive with ID: {file_id}\")\n        return file_id\n        \n    except Exception as e:\n        print(f\"Error uploading to Drive: {str(e)}\")\n        raise\n\ndef download_file_from_drive(file_id: str, destination_path: str) -> bool:\n    \"\"\"\n    Download a file from Google Drive.\n    \n    Args:\n        file_id: Google Drive file ID\n        destination_path: Local path to save the file\n        \n    Returns:\n        True if successful, False otherwise\n    \"\"\"\n    try:\n        service = get_drive_service()\n        \n        request = service.files().get_media(fileId=file_id)\n        \n        fh = io.BytesIO()\n        downloader = MediaIoBaseDownload(fh, request)\n        \n        done = False\n        while not done:\n            status, done = downloader.next_chunk()\n            print(f\"Download progress: {int(status.progress() * 100)}%\")\n        \n        # Write to file\n        with open(destination_path, 'wb') as f:\n            f.write(fh.getvalue())\n        \n        print(f\"Downloaded file from Drive to: {destination_path}\")\n        return True\n        \n    except Exception as e:\n        print(f\"Error downloading from Drive: {str(e)}\")\n        return False\n\ndef delete_file_from_drive(file_id: str) -> bool:\n    \"\"\"\n    Delete a file from Google Drive.\n    \n    Args:\n        file_id: Google Drive file ID\n        \n    Returns:\n        True if successful, False otherwise\n    \"\"\"\n    try:\n        service = get_drive_service()\n        service.files().delete(fileId=file_id).execute()\n        print(f\"Deleted file from Drive: {file_id}\")\n        return True\n    except Exception as e:\n        print(f\"Error deleting from Drive: {str(e)}\")\n        return False\n\ndef list_files_in_folder(folder_id: str, mime_type: Optional[str] = None) -> list:\n    \"\"\"\n    List all files in a specific Google Drive folder.\n    \n    Args:\n        folder_id: Google Drive folder ID\n        mime_type: Optional MIME type filter (e.g., 'image/jpeg', 'image/png')\n        \n    Returns:\n        List of file dictionaries with 'id', 'name', and 'mimeType'\n    \"\"\"\n    try:\n        service = get_drive_service()\n        \n        query = f\"'{folder_id}' in parents and trashed=false\"\n        if mime_type:\n            query += f\" and mimeType='{mime_type}'\"\n        else:\n            query += \" and (mimeType contains 'image/')\"\n        \n        results = service.files().list(\n            q=query,\n            fields=\"files(id, name, mimeType)\",\n            pageSize=1000\n        ).execute()\n        \n        files = results.get('files', [])\n        print(f\"Found {len(files)} files in folder {folder_id}\")\n        return files\n        \n    except Exception as e:\n        print(f\"Error listing files in folder: {str(e)}\")\n        return []\n\ndef download_file_to_memory(file_id: str) -> Optional[bytes]:\n    \"\"\"\n    Download a file from Google Drive to memory.\n    \n    Args:\n        file_id: Google Drive file ID\n        \n    Returns:\n        File content as bytes, or None if failed\n    \"\"\"\n    try:\n        service = get_drive_service()\n        \n        request = service.files().get_media(fileId=file_id)\n        \n        fh = io.BytesIO()\n        downloader = MediaIoBaseDownload(fh, request)\n        \n        done = False\n        while not done:\n            status, done = downloader.next_chunk()\n        \n        return fh.getvalue()\n        \n    except Exception as e:\n        print(f\"Error downloading file to memory: {str(e)}\")\n        return None\n","size_bytes":6395}},"version":1}